<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>PawSense AI – Visualizations, Results & Takeaways</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>
  <!-- Shared navigation -->
  <nav>
    <div class="nav-inner">
      <div class="nav-brand">
  <img src="assets/PawSense_logo.png" alt="PawSense AI Logo" class="nav-logo">
  PawSense AI
</div>

      <div class="nav-links">
        <a href="index.html#problem">Problem</a>
        <a href="approach.html">AlexNet Approach</a>
        <a href="results.html" class="active">Results &amp; Takeaways</a>
      </div>
    </div>
  </nav>

  <div class="page-wrapper">

    <!-- Hero -->
    <div class="hero">
      <h1>Visualizations, Results &amp; Takeaways</h1>
      <p class="subtitle">
        Training curves, sweep experiments, and key lessons from the PawSense AI project.
      </p>
    </div>

    <!-- ================================
         VISUALIZATIONS
    ================================= -->
    <div class="section-card" id="visualizations">
      <h2>Visualizations</h2>

      <p>
        These plots summarize how different training configurations behaved over time and
        how well the model generalized to validation data.
      </p>

      <h3>Training Accuracy Across Runs</h3>
      <figure>
        <img src="assets/train_acc.png" alt="Training accuracy across hyperparameter configurations">
        <figcaption>
          Training accuracy quickly approaches 0.95–0.99 for most runs. GPU + RGB (AlexNet pretrained)
          converges fastest and reaches the highest training accuracy.
        </figcaption>
      </figure>

      <h3>Validation Accuracy Across Runs</h3>
      <figure>
        <img src="assets/val_acc.png" alt="Validation accuracy across hyperparameter configurations">
        <figcaption>
          Validation accuracy peaks around 0.94–0.95 for the best configuration. Grayscale and SGD runs
          show lower peaks (≈0.85–0.90) and more fluctuation, indicating weaker generalization.
        </figcaption>
      </figure>

      <h3>Validation Loss Across Runs</h3>
      <figure>
        <img src="assets/val_loss.png" alt="Validation loss across hyperparameter configurations">
        <figcaption>
          Strong configurations stabilize at low validation loss (~0.17–0.22). Grayscale and SGD-based models
          exhibit higher and more volatile loss values, consistent with poorer validation accuracy.
        </figcaption>
      </figure>

      <h3>Best Validation Accuracy by Configuration</h3>
      <figure>
        <img src="assets/best_val_acc.png" alt="Bar chart of best validation accuracy by configuration">
        <figcaption>
          RGB + GPU + Adam (pretrained AlexNet) is the clear top performer with ≈0.95 validation accuracy.
          CPU and grayscale configurations lag behind, and SGD does not match Adam’s performance.
        </figcaption>
      </figure>
    </div>

    <!-- ================================
         RESULTS
    ================================= -->
    <div class="section-card" id="results">
      <h2>Results</h2>

      <p>
        A Weights &amp; Biases sweep was used to compare device (CPU vs GPU), color mode
        (RGB vs grayscale), and optimizer (Adam vs SGD). The experiments consistently show
        that pretrained AlexNet with RGB input, Adam optimization, and GPU acceleration
        provides the strongest and most stable performance.
      </p>

      <!-- Metric summary box -->
      <div class="metric-box">
        <h3>Performance Summary (Best Model)</h3>
        <ul>
          <li><strong>Best Training Accuracy:</strong> 0.99</li>
          <li><strong>Best Validation Accuracy:</strong> 0.95</li>
          <li><strong>Validation Loss (best epoch):</strong> ~0.17–0.22</li>
          <li><strong>Best Epoch:</strong> ~Epoch 8–10</li>
          <li><strong>Best Configuration:</strong> AlexNet (pretrained) + RGB + Adam (lr = 1e-4), Batch 32, GPU</li>
        </ul>
      </div>

      <!-- Academic-style results table -->
      <h3>Comparison of Training Configurations</h3>
      <p>The table below summarizes the relative performance of the main configurations tested.</p>

      <table>
        <thead>
          <tr>
            <th>Configuration</th>
            <th>Details</th>
            <th>Best Validation Accuracy</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>RGB + GPU + Adam</strong></td>
            <td>Pretrained AlexNet, lr = 1e-4, batch = 32</td>
            <td><strong>≈ 0.95</strong></td>
          </tr>
          <tr>
            <td>RGB + CPU + Adam</td>
            <td>Same hyperparameters, CPU device</td>
            <td>≈ 0.92</td>
          </tr>
          <tr>
            <td>Grayscale + GPU (Adam)</td>
            <td>Grayscale input instead of RGB</td>
            <td>≈ 0.88</td>
          </tr>
          <tr>
            <td>RGB + GPU + SGD</td>
            <td>Pretrained AlexNet, SGD optimizer (lr = 1e-3)</td>
            <td>≈ 0.90</td>
          </tr>
          <tr>
            <td>Grayscale + CPU (Adam)</td>
            <td>Baseline grayscale configuration on CPU</td>
            <td>≈ 0.86</td>
          </tr>
        </tbody>
      </table>

      <h3>Interpretation</h3>

      <p>
        The best-performing model is the RGB + GPU + Adam configuration. RGB images preserve color and
        texture cues (e.g., eye redness, tongue color, fur contrast) that are relevant to dog emotion;
        converting to grayscale removes these signals and consistently reduces accuracy.
      </p>

      <p>
        GPU training not only speeds up experimentation but also produces slightly more stable validation
        curves compared with CPU runs. This may be due to more consistent numerical behavior for batch
        operations and better handling of larger mini-batches.
      </p>

      <p>
        Adam outperforms SGD in this setting: SGD runs learn more slowly and show higher and more variable
        validation loss. Adam’s adaptive learning rates allow the model to converge quickly to a good
        solution without excessive tuning of the learning rate schedule.
      </p>
    </div>

    <!-- ================================
         TAKEAWAYS
    ================================= -->
    <div class="section-card" id="takeaways">
      <h2>Key Takeaways &amp; Next Steps</h2>

      <ul>
        <li>
          <strong>AlexNet transfer learning works well</strong> for binary dog emotion classification,
          achieving ≈0.95 validation accuracy on a modest-size dataset.
        </li>
        <li>
          <strong>Color information matters:</strong> RGB inputs significantly outperform grayscale,
          suggesting that many emotional cues are encoded in color and subtle shading.
        </li>
        <li>
          <strong>Adam + GPU is the preferred training setup,</strong> providing fast convergence and
          stable generalization.
        </li>
        <li>
          <strong>Overfitting is present but manageable</strong> through dropout, data augmentation,
          and early stopping.
        </li>
        <li>
          <strong>Future work:</strong> expand beyond a binary label (happy vs stressed) to a richer
          set of emotional states, incorporate more breeds and environments, and explore video-based
          models that use temporal information (e.g., tail wagging, pacing) instead of single frames.
        </li>
      </ul>

      <p>
        Overall, these experiments demonstrate that even with a relatively small dataset, a carefully
        tuned transfer-learning pipeline can capture meaningful signals about canine emotional state
        from everyday images.
      </p>
    </div>

    <footer>
      PawSense AI · Visualizations, Results &amp; Takeaways
    </footer>

  </div>
</body>
</html>
