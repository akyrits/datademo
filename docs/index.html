<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PawSense AI</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

<!-- Navigation -->
<nav>
  <a href="index.html#problem">Problem</a>
  <a href="approach.html" class="active">AlexNet Approach</a>
  <a href="results.html">Results &amp; Takeaways</a>
</nav>


<!-- Title Section -->
<section id="title" class="padded-section">
  <h1>PawSense AI: Real-Time Dog Emotion Detection</h1>
  <p><strong>Author:</strong> Alexander Kyritsopoulos<br>
  

  <p>
    PawSense AI explores whether transfer learning using AlexNet can detect
    <strong>dog emotional states</strong>—specifically <em>happy/relaxed</em> 
    vs. <em>stressed/anxious</em>—directly from images. The broader goal is to 
    evaluate whether deep learning can help pet owners better understand their dog’s 
    well-being through early detection of stress.
  </p>
</section>


<!-- Problem Section -->
<section id="problem" class="padded-section">
  <h2>Problem Overview</h2>

  <p>
    Dogs frequently display emotional and physiological stress signals subtly—
    flattened ears, widened eyes, whale-eye, tucked tail, lip licking, or a frozen posture.
    Many of these cues occur long before barking, whining, or other obvious signs.
  </p>

  <p>
    Unfortunately, owners often miss these early signals. Chronic stress in dogs has been 
    linked to behavioral issues, reduced quality of life, and worsening anxiety.
  </p>

  <p>
    This creates an opportunity for a computer-vision model capable of identifying stress 
    cues in real time from everyday images.
  </p>
</section>


<!-- Motivation Section -->
<section id="motivation" class="padded-section">
  <h2>Motivation</h2>

  <ul>
    <li>Pet owners increasingly rely on cameras and smart devices to monitor pets.</li>
    <li>Stress and anxiety frequently go undetected until severe behavior emerges.</li>
    <li>No widely available model exists that can distinguish dog emotional states 
        from simple RGB images.</li>
    <li>AI-driven early detection could improve training, well-being, and owner awareness.</li>
    <li>Long-term goal: integrate into pet cameras or vet-tech workflows.</li>
  </ul>

  <div class="summary-box">
    <h3>Why This Problem Matters</h3>
    <ul>
      <li>Early stress detection could prevent worsening anxiety and behavior issues.</li>
      <li>Provides actionable insights to owners in everyday environments.</li>
      <li>Combines computer vision with a real-world, high-impact pet-care application.</li>
    </ul>
  </div>
</section>


<!-- Approach Section -->
<section id="approach" class="padded-section">
  <h2>Technical Approach: AlexNet & Transfer Learning</h2>

  <p>
    This project uses <strong>AlexNet</strong>, a classic convolutional neural network 
    originally trained on ImageNet (1.2M images). Because AlexNet is already strong at 
    recognizing general features such as edges, colors, and textures, it can be fine-tuned 
    for smaller datasets—making it ideal for dog-emotion classification.
  </p>

  <h3>What AlexNet Does</h3>
  <p>
    AlexNet processes an input image (227×227×3 RGB) through:
  </p>
  <ul>
    <li>5 convolution layers (feature extraction)</li>
    <li>Max pooling layers (downsampling)</li>
    <li>ReLU activations (fast training)</li>
    <li>Dropout (reducing overfitting)</li>
    <li>3 fully connected layers (final classification)</li>
  </ul>

  <div class="summary-box">
    <h3>Key Concepts</h3>
    <ul>
      <li><strong>Kernel:</strong> sliding window that extracts features.</li>
      <li><strong>Stride:</strong> how far the kernel moves each step.</li>
      <li><strong>ReLU:</strong> activation <code>max(0, x)</code></li>
      <li><strong>Max Pooling:</strong> keeps strongest spatial features.</li>
      <li><strong>Dropout:</strong> reduces overfitting.</li>
    </ul>
  </div>

  <h3>Training Workflow</h3>
  <ol>
    <li>Load and preprocess image (resize 227×227, normalize, augment if needed).</li>
    <li>Forward pass through AlexNet.</li>
    <li>Compute classification loss (cross-entropy).</li>
    <li>Backpropagate gradients + optimizer update (Adam/SGD).</li>
    <li>Track accuracy/loss curves with validation data.</li>
    <li>Use early stopping to prevent overfitting.</li>
  </ol>

  <p>
    Multiple training configurations were tested: RGB vs. grayscale input, pretrained vs. 
    non-pretrained weights, Adam vs. SGD, various learning rates, and batch sizes.
  </p>
</section>


<!-- Visualizations Section -->
<section id="visualizations" class="padded-section">
  <h2>Training Visualizations</h2>

  <figure>
    <img src="assets/train_val_accuracy.png" alt="Training vs validation accuracy">
    <figcaption>Training vs validation accuracy curves across epochs.</figcaption>
  </figure>

  <figure>
    <img src="assets/train_val_loss.png" alt="Training vs validation loss">
    <figcaption>Training vs validation loss curves indicating overfitting behavior.</figcaption>
  </figure>

  <figure>
    <img src="assets/confusion_matrix.png" alt="Confusion matrix for happy vs stressed dog classification">
    <figcaption>Confusion matrix showing classification performance on validation data.</figcaption>
  </figure>
</section>


<!-- Results Section -->
<section id="results" class="padded-section">
  <h2>Results Summary</h2>

  <ul>
    <li>Best training accuracy: <strong>[Fill in]</strong></li>
    <li>Best validation accuracy: <strong>[Fill in]</strong></li>
    <li>Best epoch (lowest validation loss): <strong>[Fill in]</strong></li>
    <li>Misclassifications occurred primarily in low-light or ambiguous positions.</li>
  </ul>

  <h3>Hyperparameter Experiments</h3>
  <p>A subset of tested configurations:</p>

  <table>
    <thead>
      <tr>
        <th>Run</th><th>Optimizer</th><th>Pretrained</th><th>LR</th><th>Batch</th>
        <th>Device</th><th>Val Acc</th><th>Notes</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>GPU-Adam-RGB</td><td>Adam</td><td>Yes</td><td>1e-4</td><td>32</td><td>GPU</td><td>[Fill]</td><td>Best overall performance.</td></tr>
      <tr><td>GPU-Adam-Gray</td><td>Adam</td><td>Yes</td><td>1e-4</td><td>32</td><td>GPU</td><td>[Fill]</td><td>Lower performance vs RGB.</td></tr>
      <tr><td>GPU-SGD-RGB</td><td>SGD</td><td>Yes</td><td>1e-3</td><td>32</td><td>GPU</td><td>[Fill]</td><td>Slower convergence.</td></tr>
      <tr><td>CPU-Adam-RGB</td><td>Adam</td><td>Yes</td><td>1e-4</td><td>32</td><td>CPU</td><td>[Fill]</td><td>Similar accuracy; much slower.</td></tr>
      <tr><td>GPU-Adam-NoPre</td><td>Adam</td><td>No</td><td>1e-4</td><td>32</td><td>GPU</td><td>[Fill]</td><td>Underfitting; benefits of pretrained weights.</td></tr>
    </tbody>
  </table>

</section>


<!-- Takeaways Section -->
<section id="takeaways" class="padded-section">
  <h2>Project Takeaways</h2>

  <ul>
    <li>AlexNet transfer learning captures meaningful features for dog emotional cues.</li>
    <li>Pretrained RGB models perform best, with clearer separation between classes.</li>
    <li>Misclassified cases suggest lighting, angle, and subtle cues still challenge the model.</li>
    <li>Larger and more diverse datasets would significantly improve generalization.</li>
    <li>Future direction: real-time deployment inside pet cameras or mobile apps.</li>
  </ul>
</section>


<footer>
  <p>© 2025 PawSense AI · Built with HTML/CSS on GitHub Pages</p>
</footer>

</body>
</html>
