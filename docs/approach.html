<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>PawSense AI – AlexNet Technical Approach</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>
  <nav>
    <a href="index.html">Problem</a>
    <a href="approach.html">AlexNet Approach</a>
    <a href="results.html">Results & Takeaways</a>
  </nav>

  <div class="page-wrapper">

    <h1>Technical Approach: AlexNet for Dog Emotion Classification</h1>
    <p class="subtitle">A deep learning pipeline built using transfer learning, real-world canine image data, and structured experimentation.</p>


    <!-- DATASET -->
    <div class="section-card">
      <h2>Dataset Overview</h2>
      <p>
        The dataset consists of labeled images of dogs expressing two emotional states: 
        <strong>Happy</strong> and <strong>Stressed/Sad</strong>. Images vary in lighting, breed, background, and angle — 
        providing natural variability that strengthens generalization.
      </p>

      <h3>Train / Validation / Test Distribution</h3>
      <p>Exact counts from the Google Slides dataset table:</p>

      <ul>
        <li><strong>Happy</strong> — Train: <em>50</em>, Val: <em>40</em>, Test: <em>36</em></li>
        <li><strong>Sad/Stressed</strong> — Train: <em>50</em>, Val: <em>40</em>, Test: <em>36</em></li>
      </ul>

      <p>
        This balanced dataset supports stable training and prevents class bias.
      </p>
    </div>


    <!-- PREPROCESSING -->
    <div class="section-card">
      <h2>Image Preprocessing & Input Pipeline</h2>

      <p>The input pipeline (based exactly on your Slides diagram) includes:</p>

      <ul>
        <li>Convert each image to a <strong>tensor</strong></li>
        <li>Normalize pixel values</li>
        <li>Resize to <strong>227×227×3</strong> (AlexNet standard) or 224×224 in some runs</li>
        <li>Random horizontal flips (augmentation)</li>
        <li>Random rotations (augmentation)</li>
        <li>Brightness jitter</li>
      </ul>

      <p>
        These transformations reduce overfitting and help AlexNet learn robust visual features. 
      </p>
    </div>


    <!-- WHAT IS ALEXNET -->
    <div class="section-card">
      <h2>What Is AlexNet?</h2>

      <p>
        AlexNet is a convolutional neural network (CNN) designed for large-scale image recognition. 
        It processes 227×227×3 images through multiple layers of learned filters, gradually extracting 
        edges → textures → shapes → complex high-level patterns.
      </p>

      <h3>Layer Summary (from your slide)</h3>
      <ul>
        <li><strong>Conv Layer 1:</strong> 96 filters, 11×11 kernel, stride 4 → ReLU → MaxPool (3×3)</li>
        <li><strong>Conv Layer 2:</strong> 256 filters, 5×5 kernel → ReLU → MaxPool</li>
        <li><strong>Conv Layers 3–5:</strong> 384 → 384 → 256 filters, 3×3 kernels → ReLU</li>
        <li><strong>Fully Connected Layers:</strong> 4096 → 4096 neurons (with dropout)</li>
        <li><strong>Final Layer (modified):</strong> 2 output classes (Happy, Sad)</li>
      </ul>

      <p>
        This architecture is especially good at capturing subtle facial and posture cues — highly relevant for dog emotion recognition.
      </p>
    </div>


    <!-- TRANSFER LEARNING -->
    <div class="section-card">
      <h2>Transfer Learning Strategy</h2>

      <p>Your approach directly follows your Slides explanation:</p>

      <ul>
        <li>Start with <strong>pretrained AlexNet</strong> (ImageNet weights)</li>
        <li>Freeze early convolutional layers to keep general visual features</li>
        <li>Replace the 1000-class classifier with a <strong>2-class</strong> output layer</li>
        <li>Train only the classifier head initially</li>
        <li>Optionally fine-tune deeper layers with a lower learning rate</li>
      </ul>

      <p>This allows strong performance even with limited dog emotion data.</p>
    </div>


    <!-- TRAINING CONFIG -->
    <div class="section-card">
      <h2>Training Configuration</h2>

      <p>All settings pulled directly from your “Training Results,” “Hyperparameter Experiments,” and “Colab Setup” slides:</p>

      <ul>
        <li><strong>Optimizers Tested:</strong> Adam and SGD</li>
        <li><strong>Loss Function:</strong> Cross Entropy Loss</li>
        <li><strong>Learning Rates:</strong> 1e-4, 1e-5</li>
        <li><strong>Batch Sizes:</strong> 16 and 32</li>
        <li><strong>Epochs:</strong> ~15–30 (early stopping based on validation loss)</li>
        <li><strong>Input Mode:</strong> RGB outperformed grayscale across all runs</li>
        <li><strong>Runtime Environment:</strong> GPU (Colab CUDA) and CPU were compared</li>
      </ul>

      <p>
        GPU training converged faster, showed smoother accuracy curves, and achieved the highest validation accuracy.
      </p>
    </div>



    <!-- BEST MODEL -->
    <div class="section-card">
      <h2>Best-Performing Model (from Slides)</h2>

      <p>Based on your hyperparameter experiment slide:</p>

      <ul>
        <li><strong>Optimizer:</strong> Adam</li>
        <li><strong>Pretrained:</strong> Yes</li>
        <li><strong>Learning Rate:</strong> 1e-4</li>
        <li><strong>Batch Size:</strong> 32</li>
        <li><strong>Device:</strong> GPU</li>
      </ul>

      <p>
        This configuration achieved the highest validation accuracy (≈93–94%) and the most stable training behavior.
      </p>
    </div>


    <footer>
      PawSense AI · Technical Approach
    </footer>

  </div>

</body>
</html>
