<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>PawSense AI – AlexNet Technical Approach</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>
  <nav>
    <a href="index.html">Problem</a>
    <a href="approach.html">AlexNet Approach</a>
    <a href="results.html">Results & Takeaways</a>
  </nav>

  <div class="page-wrapper">

    <h1>Technical Approach: AlexNet for Dog Emotion Recognition</h1>
    <p class="subtitle">
      How raw dog images become “happy vs. stressed” predictions using transfer learning.
    </p>


    <!-- SAMPLE IMAGES -->
    <div class="section-card">
      <h2>What the Model Sees</h2>
      <p>Below are examples of the types of images used during training:</p>

      <div class="image-row">
        <img src="assets/sample_happy.jpg" class="img-small" alt="Happy dog example" />
        <img src="assets/sample_stressed.jpg" class="img-small" alt="Stressed dog example" />
      </div>
    </div>



    <!-- DATASET -->
    <div class="section-card">
      <h2>Dataset & Labeling</h2>

      <p>
        For this first version of PawSense AI, I built a small but balanced dataset of dog images.
        Each image is labeled as either <strong>Happy</strong> or <strong>Stressed/Sad</strong> based on
        visible body language and facial cues (ears, eyes, mouth, posture).
      </p>

      <p>
        The data is split into training, validation, and test sets so I can tune hyperparameters 
        without “peeking” at the final performance.
      </p>

      <table>
        <thead>
          <tr>
            <th>Class</th>
            <th>Train</th>
            <th>Validation</th>
            <th>Test</th>
            <th>Total</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Happy</td>
            <td>50</td>
            <td>40</td>
            <td>36</td>
            <td>126</td>
          </tr>
          <tr>
            <td>Stressed / Sad</td>
            <td>50</td>
            <td>40</td>
            <td>36</td>
            <td>126</td>
          </tr>
          <tr class="total-row">
            <td><strong>All images</strong></td>
            <td><strong>100</strong></td>
            <td><strong>80</strong></td>
            <td><strong>72</strong></td>
            <td><strong>252</strong></td>
          </tr>
        </tbody>
      </table>

      <p>
        This balanced setup makes it easier to interpret accuracy and helps prevent the model 
        from “cheating” by always predicting the majority class.
      </p>
    </div>



    <!-- PREPROCESSING -->
    <div class="section-card">
      <h2>Image Preprocessing Pipeline</h2>
      <p>
        Before entering AlexNet, each image goes through a structured preprocessing pipeline:
      </p>

      <img src="assets/preprocessing_pipeline.png" class="img-wide" alt="Preprocessing pipeline diagram" />

      <ul>
        <li>Resize to 227×227×3</li>
        <li>Normalize to match ImageNet statistics</li>
        <li>Convert to PyTorch tensor</li>
        <li>Data augmentation:
          <ul>
            <li>Horizontal flips</li>
            <li>Small rotations</li>
            <li>Brightness jitter</li>
          </ul>
        </li>
      </ul>
    </div>


    <!-- ARCHITECTURE -->
    <div class="section-card">
      <h2>AlexNet Architecture (Simplified Diagram)</h2>

      <img src="assets/alexnet_architecture.png" class="img-wide" alt="AlexNet Architecture Diagram" />

      <p>
        AlexNet is composed of 5 convolutional layers followed by 3 fully connected layers.
        For this project, it serves as a powerful feature extractor that identifies facial tension,
        ears, eyes, shape cues, and overall posture.
      </p>
    </div>


    <!-- TRANSFER LEARNING -->
    <div class="section-card">
      <h2>Transfer Learning Strategy</h2>

      <p>
        Instead of training AlexNet from scratch, the model starts with pretrained ImageNet weights.
        This gives it strong general vision features before learning canine emotional cues.
      </p>

      <ul>
        <li>Load pretrained AlexNet</li>
        <li>Freeze early convolution layers</li>
        <li>Replace final classifier with a custom 2-class head</li>
        <li>Train classifier first, then fine-tune deeper layers</li>
      </ul>

      <p>
        This approach significantly improves accuracy on small datasets and reduces overfitting.
      </p>
    </div>


    <!-- TRAINING CONFIG -->
    <div class="section-card">
      <h2>Training Configuration</h2>

      <p>
        Training was performed in Google Colab on both CPU and GPU to compare performance.
      </p>

      <ul>
        <li><strong>Optimizers:</strong> Adam, SGD</li>
        <li><strong>Loss:</strong> Cross Entropy</li>
        <li><strong>Batch Sizes:</strong> 16 and 32</li>
        <li><strong>Learning Rates:</strong> 1e-4, 1e-5</li>
        <li><strong>Epochs:</strong> ~15–30</li>
        <li><strong>Input:</strong> RGB (performed better than grayscale)</li>
        <li><strong>Best device:</strong> GPU (more stable training curves)</li>
      </ul>
    </div>


    <!-- BEST MODEL -->
    <div class="section-card">
      <h2>Best Model Configuration</h2>

      <p>
        After testing multiple setups, the strongest configuration was:
      </p>

      <ul>
        <li>Pretrained AlexNet backbone</li>
        <li>Custom 2-class classifier head</li>
        <li>Optimizer: Adam</li>
        <li>Learning Rate: 1e-4</li>
        <li>Batch Size: 32</li>
        <li>Device: GPU</li>
      </ul>

      <p>
        This combination yielded the highest validation accuracy and cleanest convergence curve.
      </p>
    </div>

    <footer>
      PawSense AI · Technical Approach
    </footer>

  </div>
</body>
</html>
