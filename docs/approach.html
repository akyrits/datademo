<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>PawSense AI – AlexNet Technical Approach</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>
<nav>
  <a href="index.html#problem">Problem</a>
  <a href="approach.html" class="active">AlexNet Approach</a>
  <a href="results.html">Results &amp; Takeaways</a>
</nav>



  <div class="page-wrapper">

    <div class="hero">
      <h1>Technical Approach: AlexNet for Dog Emotion Recognition</h1>
      <p class="subtitle">
        How raw dog images become “happy vs. stressed” predictions using transfer learning.
      </p>
    </div>


    <!-- SAMPLE IMAGES -->
    <div class="section-card">
      <h2>What the Model Sees</h2>
      <p>Below are examples of the types of images used during training:</p>

      <div class="image-row">
        <img src="assets/sample_happy1.jpg" class="img-small" alt="Happy dog example" />
        <img src="assets/sample_stressed1.jpg" class="img-small img-free" alt="Stressed dog example" />
      </div>
    </div>

    <!-- DATASET -->
    <div class="section-card">
      <h2>Dataset & Labeling</h2>

      <p>
        For this first version of PawSense AI, I built a small but balanced dataset of dog images.
        Each image is labeled as either <strong>Happy</strong> or <strong>Stressed/Sad</strong>
        based on visible body language and facial cues (ears, eyes, mouth, posture).
      </p>

      <p>
        The data is split into training, validation, and test sets so I can tune hyperparameters
        without “peeking” at the final performance.
      </p>

      <table>
        <thead>
          <tr>
            <th>Class</th>
            <th>Train</th>
            <th>Validation</th>
            <th>Test</th>
            <th>Total</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Happy</td>
            <td>50</td>
            <td>40</td>
            <td>36</td>
            <td>126</td>
          </tr>
          <tr>
            <td>Stressed / Sad</td>
            <td>50</td>
            <td>40</td>
            <td>36</td>
            <td>126</td>
          </tr>
          <tr class="total-row">
            <td><strong>All images</strong></td>
            <td><strong>100</strong></td>
            <td><strong>80</strong></td>
            <td><strong>72</strong></td>
            <td><strong>252</strong></td>
          </tr>
        </tbody>
      </table>

      <p>
        This balanced setup makes it easier to interpret accuracy and helps prevent the model
        from “cheating” by always predicting the majority class.
      </p>
    </div>

    <!-- PREPROCESSING -->
    <div class="section-card">
      <h2>Image Preprocessing Pipeline</h2>
      <p>
        Before entering AlexNet, each image goes through a structured preprocessing pipeline:
      </p>

      <div class="pipeline">
        <div class="pipeline-step">Raw dog image</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Resize to 227×227</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Normalize (ImageNet stats)</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Convert to tensor</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Augment (flip/rotate/brightness)</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Feed into AlexNet</div>
      </div>

      <ul>
        <li><strong>Resize:</strong> images are resized to 227×227×3.</li>
        <li><strong>Normalization:</strong> pixel values are scaled to match ImageNet statistics.</li>
        <li><strong>Tensor conversion:</strong> images are converted into PyTorch tensors.</li>
        <li><strong>Augmentation:</strong> random flips, small rotations, and brightness jitter to improve robustness.</li>
      </ul>
    </div>

    <!-- ARCHITECTURE -->
    <div class="section-card">
      <h2>AlexNet Architecture (Short Version)</h2>

      <img src="assets/alexnet_architecture.png" class="img-wide" alt="AlexNet Architecture Diagram" />

      <p>
        AlexNet is a convolutional neural network originally designed for large-scale image
        classification. In this project, it serves as a feature extractor for canine facial
        expressions and body cues.
      </p>

      <h3>Feature Extractor</h3>
      <ul>
        <li><strong>Conv1:</strong> 96 filters, 11×11 kernel, stride 4 → ReLU → MaxPool</li>
        <li><strong>Conv2:</strong> 256 filters, 5×5 kernel → ReLU → MaxPool</li>
        <li><strong>Conv3:</strong> 384 filters, 3×3 kernel → ReLU</li>
        <li><strong>Conv4:</strong> 384 filters, 3×3 kernel → ReLU</li>
        <li><strong>Conv5:</strong> 256 filters, 3×3 kernel → ReLU → MaxPool</li>
      </ul>

      <h3>Classifier Head (Modified)</h3>
      <ul>
        <li>Flatten features from the last conv layer.</li>
        <li>Fully connected layer → ReLU → Dropout.</li>
        <li>Fully connected layer → ReLU → Dropout.</li>
        <li>Final fully connected layer with <strong>2 outputs</strong> (Happy, Stressed).</li>
      </ul>
    </div>

    <!-- TRANSFER LEARNING -->
    <div class="section-card">
      <h2>Transfer Learning Strategy</h2>
      <p>
        Training a deep CNN from scratch on a small dog-emotion dataset would overfit quickly.
        Instead, this project uses transfer learning with AlexNet pretrained on ImageNet.
      </p>
      <ul>
        <li>Initialize the model with pretrained ImageNet weights.</li>
        <li>Use the convolutional stack as a fixed or lightly fine-tuned feature extractor.</li>
        <li>Replace the original 1000-class output layer with a new 2-class classifier.</li>
        <li>Train the classifier layers first, then optionally fine-tune deeper layers with a smaller learning rate.</li>
      </ul>
      <p>
        This approach leverages generic visual features (edges, textures, shapes) learned from millions
        of images and adapts them to the “happy vs stressed” dog dataset.
      </p>
    </div>

    <!-- TRAINING CONFIG -->
    <div class="section-card">
      <h2>Training Configuration</h2>
      <p>
        I experimented with several training setups in Google Colab, focusing on how optimizer,
        learning rate, batch size, and device (CPU vs GPU) affected performance.
      </p>

      <h3>Core Settings</h3>
      <ul>
        <li><strong>Loss:</strong> Cross-entropy loss.</li>
        <li><strong>Optimizers tested:</strong> Adam and SGD.</li>
        <li><strong>Batch sizes:</strong> 16 and 32.</li>
        <li><strong>Learning rates:</strong> 1e-4 and 1e-5 for fine-tuning.</li>
        <li><strong>Epochs:</strong> ~15–30 with early stopping based on validation loss.</li>
        <li><strong>Input mode:</strong> RGB images (grayscale variants performed worse).</li>
        <li><strong>Hardware:</strong> CPU and GPU (CUDA) runs for comparison.</li>
      </ul>
    </div>

    <!-- BEST MODEL -->
    <div class="section-card">
      <h2>Best-Performing Configuration</h2>
      <p>
        The final go-to configuration for this project is:
      </p>
      <ul>
        <li>Pretrained AlexNet backbone.</li>
        <li>Custom 2-class classifier head.</li>
        <li>Optimizer: <strong>Adam</strong>.</li>
        <li>Learning rate: <strong>1e-4</strong>.</li>
        <li>Batch size: <strong>32</strong>.</li>
        <li>Device: <strong>GPU</strong> (Colab CUDA).</li>
      </ul>
      <p>
        This setup achieved the best combination of high validation accuracy, fast convergence,
        and a relatively small gap between training and validation performance.
      </p>
    </div>

    <footer>
      PawSense AI · AlexNet Technical Approach
    </footer>

  </div>
</body>
</html>
