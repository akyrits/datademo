<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>PawSense AI – AlexNet Technical Approach</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>
  <nav>
    <a href="index.html">Problem</a>
    <a href="approach.html">AlexNet Approach</a>
    <a href="results.html">Results & Takeaways</a>
  </nav>

  <div class="page-wrapper">

    <h1>Technical Approach: AlexNet for Dog Emotion Classification</h1>
    <p class="subtitle">
      From raw dog photos to “happy vs. stressed” predictions using transfer learning.
    </p>

    <!-- HIGH LEVEL PIPELINE -->
    <div class="section-card">
      <h2>High-Level Pipeline</h2>
      <p>
        The model follows a simple end-to-end workflow:
      </p>
      <ol>
        <li>Start with a real-world dog image (happy or stressed).</li>
        <li>Apply preprocessing: resize, normalize, and augment.</li>
        <li>Feed the image into a pretrained AlexNet feature extractor.</li>
        <li>Use a custom classifier head to predict <strong>Happy</strong> or <strong>Stressed</strong>.</li>
        <li>Evaluate performance on a held-out validation/test set.</li>
      </ol>
      <p>
        The rest of this page explains each piece in more detail.
      </p>
    </div>

    <!-- DATASET -->
    <div class="section-card">
      <h2>Dataset & Labels</h2>
      <p>
        The dataset is a balanced collection of dog images labeled into two classes:
        <strong>Happy</strong> and <strong>Sad/Stressed</strong>. Images come from a mix of
        environments, lighting conditions, and breeds so the model doesn’t overfit to one “type” of dog.
      </p>

      <h3>Split by Class</h3>
      <table>
        <thead>
          <tr>
            <th>Class</th>
            <th>Train</th>
            <th>Validation</th>
            <th>Test</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Happy</td>
            <td>50</td>
            <td>40</td>
            <td>36</td>
          </tr>
          <tr>
            <td>Sad / Stressed</td>
            <td>50</td>
            <td>40</td>
            <td>36</td>
          </tr>
        </tbody>
      </table>

      <p>
        Keeping the classes balanced makes it easier to interpret accuracy and prevents the model
        from “cheating” by always predicting the majority class.
      </p>
    </div>

    <!-- PREPROCESSING -->
    <div class="section-card">
      <h2>Image Preprocessing</h2>
      <p>
        Before images are fed into AlexNet, they pass through a preprocessing pipeline:
      </p>
      <ul>
        <li><strong>Resize:</strong> images are resized to 227×227×3 (AlexNet’s original input size).</li>
        <li><strong>Normalization:</strong> pixel values are scaled and normalized to match ImageNet statistics.</li>
        <li><strong>Tensor conversion:</strong> images are converted into PyTorch tensors.</li>
        <li><strong>Data augmentation:</strong>
          <ul>
            <li>Random horizontal flips</li>
            <li>Small random rotations</li>
            <li>Brightness jitter</li>
          </ul>
        </li>
      </ul>
      <p>
        Augmentation helps the model generalize to different lighting, poses, and camera angles
        that real dogs will appear in.
      </p>
    </div>

    <!-- ARCHITECTURE -->
    <div class="section-card">
      <h2>AlexNet Architecture (Short Version)</h2>
      <p>
        AlexNet is a convolutional neural network originally designed for large-scale image
        classification. In this project, it serves as a powerful feature extractor for canine
        facial expressions and body cues.
      </p>

      <h3>Feature Extractor</h3>
      <ul>
        <li><strong>Conv1:</strong> 96 filters, 11×11 kernel, stride 4 → ReLU → MaxPool</li>
        <li><strong>Conv2:</strong> 256 filters, 5×5 kernel → ReLU → MaxPool</li>
        <li><strong>Conv3:</strong> 384 filters, 3×3 kernel → ReLU</li>
        <li><strong>Conv4:</strong> 384 filters, 3×3 kernel → ReLU</li>
        <li><strong>Conv5:</strong> 256 filters, 3×3 kernel → ReLU → MaxPool</li>
      </ul>

      <h3>Classifier Head (Modified)</h3>
      <ul>
        <li>Flatten features from the last conv layer</li>
        <li>Fully connected layer → ReLU → Dropout</li>
        <li>Fully connected layer → ReLU → Dropout</li>
        <li>Final fully connected layer with <strong>2 outputs</strong> (Happy, Stressed)</li>
      </ul>

      <p>
        The convolutional layers learn reusable visual features; the final layers are adapted
        specifically for dog emotion recognition.
      </p>
    </div>

    <!-- TRANSFER LEARNING -->
    <div class="section-card">
      <h2>Transfer Learning Strategy</h2>
      <p>
        Training a deep CNN from scratch on a small dog-emotion dataset would overfit quickly.
        Instead, the project uses transfer learning with AlexNet pretrained on ImageNet.
      </p>
      <ul>
        <li>Initialize the model with pretrained ImageNet weights.</li>
        <li>Use the convolutional stack as a fixed (or lightly fine-tuned) feature extractor.</li>
        <li>Replace the original 1000-class output layer with a new 2-class classifier.</li>
        <li>Train the new classifier layers first, then optionally fine-tune deeper layers with a smaller learning rate.</li>
      </ul>
      <p>
        This approach leverages generic visual features (edges, textures, shapes) learned from millions
        of images and adapts them to the much smaller “happy vs stressed” dog dataset.
      </p>
    </div>

    <!-- TRAINING CONFIG -->
    <div class="section-card">
      <h2>Training Configuration</h2>
      <p>
        I experimented with several training setups in Google Colab, focusing on how optimizer,
        learning rate, batch size, and device (CPU vs GPU) affected performance.
      </p>

      <h3>Core Settings</h3>
      <ul>
        <li><strong>Loss:</strong> Cross-entropy loss</li>
        <li><strong>Optimizers tested:</strong> Adam and SGD</li>
        <li><strong>Batch sizes:</strong> 16 and 32</li>
        <li><strong>Learning rates:</strong> 1e-4 and 1e-5 for fine-tuning</li>
        <li><strong>Epochs:</strong> ~15–30 with early stopping based on validation loss</li>
        <li><strong>Input mode:</strong> RGB images (grayscale variants performed worse)</li>
        <li><strong>Hardware:</strong> CPU and GPU (CUDA) runs for comparison</li>
      </ul>

      <p>
        Overall, RGB inputs with the Adam optimizer, a learning rate around 1e-4, and a batch size of 32
        on GPU produced the most stable training curves and highest validation accuracy.
      </p>
    </div>

    <!-- BEST MODEL -->
    <div class="section-card">
      <h2>Best-Performing Configuration</h2>
      <p>
        The final “go-to” configuration for this project is:
      </p>
      <ul>
        <li>Pretrained AlexNet backbone</li>
        <li>Custom 2-class classifier head</li>
        <li>Optimizer: <strong>Adam</strong></li>
        <li>Learning rate: <strong>1e-4</strong></li>
        <li>Batch size: <strong>32</strong></li>
        <li>Device: <strong>GPU</strong> (Colab CUDA)</li>
      </ul>
      <p>
        This setup achieved the best combination of high validation accuracy, fast convergence,
        and relatively small gap between training and validation performance.
      </p>
    </div>

    <footer>
      PawSense AI · AlexNet Technical Approach
    </footer>

  </div>
</body>
</html>
