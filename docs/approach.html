<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>PawSense AI – AlexNet Technical Approach</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>

  <!-- Shared navigation -->
  <nav>
    <div class="nav-inner">
      <div class="nav-brand">PawSense AI</div>
      <div class="nav-links">
        <a href="index.html#problem">Problem</a>
        <a href="approach.html" class="active">AlexNet Approach</a>
        <a href="results.html">Results &amp; Takeaways</a>
      </div>
    </div>
  </nav>

  <div class="page-wrapper">

    <!-- Hero -->
    <div class="hero">
      <h1>Technical Approach: AlexNet for Dog Emotion Recognition</h1>
      <p class="subtitle">
        How raw dog images become “happy vs. stressed” predictions using transfer learning.
      </p>
    </div>

    <!-- SAMPLE IMAGES -->
    <div class="section-card" id="samples">
      <h2>What the Model Sees</h2>
      <p>
        The model is trained on images of dogs labeled as <strong>Happy</strong> or
        <strong>Stressed/Sad</strong>. Below are examples of the types of images used during training:
      </p>

      <div class="image-row">
        <img src="assets/sample_happy1.jpg"
             class="img-small"
             alt="Happy dog example" />
        <img src="assets/sample_stressed1.jpg"
             class="img-small"
             alt="Stressed dog example" />
      </div>
    </div>

    <!-- DATASET -->
    <div class="section-card" id="dataset">
      <h2>Dataset &amp; Labeling</h2>

      <p>
        For this first version of PawSense AI, I built a small but balanced dataset of dog images.
        Each image is labeled as either <strong>Happy</strong> or <strong>Stressed/Sad</strong>
        based on visible body language and facial cues (ears, eyes, mouth, posture).
      </p>

      <p>
        The data is split into training, validation, and test sets so I can tune hyperparameters
        without “peeking” at the final performance.
      </p>

      <table>
        <thead>
          <tr>
            <th>Class</th>
            <th>Train</th>
            <th>Validation</th>
            <th>Test</th>
            <th>Total</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Happy</td>
            <td>50</td>
            <td>40</td>
            <td>36</td>
            <td>126</td>
          </tr>
          <tr>
            <td>Stressed / Sad</td>
            <td>50</td>
            <td>40</td>
            <td>36</td>
            <td>126</td>
          </tr>
          <tr class="total-row">
            <td><strong>All images</strong></td>
            <td><strong>100</strong></td>
            <td><strong>80</strong></td>
            <td><strong>72</strong></td>
            <td><strong>252</strong></td>
          </tr>
        </tbody>
      </table>

      <p>
        This balanced setup makes it easier to interpret accuracy and helps prevent the model
        from “cheating” by always predicting the majority class.
      </p>
    </div>

    <!-- PREPROCESSING -->
    <div class="section-card">
      <h2>Image Preprocessing Pipeline</h2>

      <p>
        Before entering AlexNet, each image goes through a structured preprocessing pipeline:
      </p>

      <div class="pipeline">
        <div class="pipeline-step">Raw dog image</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Resize to 227×227</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Normalize (ImageNet stats)</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Convert to tensor</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Augment (flip/rotate/brightness)</div>
        <span class="pipeline-arrow">➜</span>
        <div class="pipeline-step">Feed into AlexNet</div>
      </div>

      <ul>
        <li><strong>Resize:</strong> images are resized to 227×227×3.</li>
        <li><strong>Normalization:</strong> pixel values are scaled to match ImageNet statistics.</li>
        <li><strong>Tensor conversion:</strong> images are converted into PyTorch tensors.</li>
        <li><strong>Augmentation:</strong> random flips, small rotations, and brightness jitter to improve robustness.</li>
      </ul>
    </div>

    <!-- ARCHITECTURE -->
    <div class="section-card">
      <h2>AlexNet Architecture (Short Version)</h2>

      <img src="assets/alexnet_architecture.png"
           class="img-wide"
           alt="AlexNet Architecture Diagram" />

      <p>
        AlexNet is a convolutional neural network originally designed for large-scale image
        classification. In this project, it serves as a feature extractor for canine facial
        expressions and body cues.
      </p>

      <h3>Feature Extractor</h3>
      <ul>
        <li><strong>Conv1:</strong> 96 filters, 11×11 kernel, stride 4 → ReLU → MaxPool</li>
        <li><strong>Conv2:</strong> 256 filters, 5×5 kernel → ReLU → MaxPool</li>
        <li><strong>Conv3:</strong> 384 filters, 3×3 kernel → ReLU</li>
        <li><strong>Conv4:</strong> 384 filters, 3×3 kernel → ReLU</li>
        <li><strong>Conv5:</strong> 256 filters, 3×3 kernel → ReLU → MaxPool</li>
      </ul>

      <h3>Classifier Head (Modified)</h3>
      <ul>
        <li>Flatten features from the last convolutional layer.</li>
        <li>Fully connected layer → ReLU → Dropout.</li>
        <li>Fully connected layer → ReLU → Dropout.</li>
        <li>Final fully connected layer with <strong>2 outputs</strong> (Happy, Stressed).</li>
      </ul>
    </div>

    <!-- KEY CONCEPTS: RELU, MAXPOOL, DROPOUT -->
    <div class="section-card">
      <h2>Key Concepts Explained Simply</h2>

      <p>
        Deep learning models like AlexNet rely on core operations that help the network
        learn patterns from images. Here are the most important ones used in this project:
      </p>

      <h3>ReLU (Rectified Linear Unit)</h3>
      <p>
        ReLU is an activation function that replaces negative values with zero:
        <code>ReLU(x) = max(0, x)</code>.
      </p>
      <p>
        In simple terms: <strong>ReLU helps the model learn faster</strong>.
        Instead of slowing down on negative numbers, it ignores them and focuses only on
        useful signals. This prevents the model from getting “stuck” and makes training
        more stable and efficient.
      </p>

      <h3>Max Pooling</h3>
      <p>
        MaxPooling reduces the size of the image representation by taking the largest value
        in a small window (like 2×2 or 3×3) as it slides across the feature map.
      </p>
      <p>
        In real terms: <strong>MaxPooling keeps the strongest part of a pattern</strong>.
        For example, if a dog's eye edge appears across multiple nearby pixels,
        MaxPool keeps only the clearest version. This makes the model:
      </p>
      <ul>
        <li>More efficient</li>
        <li>Less sensitive to small shifts in the image</li>
        <li>Better at recognizing general shapes and features</li>
      </ul>

      <h3>Dropout</h3>
      <p>
        Dropout randomly “turns off” a small percentage of neurons during training.
      </p>
      <p>
        In real terms: <strong>Dropout prevents overfitting</strong>.
        By disabling different neurons each training step, the model can't rely on a single
        pathway or memorize training examples. It is forced to learn patterns that
        generalize — the kinds of cues that truly distinguish a happy dog from a stressed one.
      </p>
    </div>

    <!-- TRANSFER LEARNING -->
    <div class="section-card">
      <h2>Transfer Learning Strategy</h2>
      <p>
        Training a deep CNN from scratch on a small dog-emotion dataset would overfit quickly.
        Instead, this project uses transfer learning with AlexNet pretrained on ImageNet.
      </p>
      <ul>
        <li>Initialize the model with pretrained ImageNet weights.</li>
        <li>Use the convolutional stack as a fixed or lightly fine-tuned feature extractor.</li>
        <li>Replace the original 1000-class output layer with a new 2-class classifier.</li>
        <li>
          Train the classifier layers first, then optionally fine-tune deeper layers with a
          smaller learning rate.
        </li>
      </ul>
      <p>
        This approach leverages generic visual features (edges, textures, shapes) learned from
        millions of images and adapts them to the “happy vs stressed” dog dataset.
      </p>
    </div>

    <!-- TRAINING CONFIGURATION -->
    <div class="section-card">
      <h2>Training Configuration</h2>
      <p>
        I experimented with several training setups in Google Colab, focusing on how the optimizer,
        learning rate, batch size, and device (CPU vs GPU) affected performance.
      </p>

      <h3>Core Settings</h3>
      <ul>
        <li><strong>Loss:</strong> Cross-entropy loss.</li>
        <li><strong>Optimizers tested:</strong> Adam and SGD.</li>
        <li><strong>Batch sizes:</strong> 16 and 32.</li>
        <li><strong>Learning rates:</strong> 1e-4 and 1e-5 for fine-tuning.</li>
        <li><strong>Epochs:</strong> ~15–30 with early stopping based on validation loss.</li>
        <li><strong>Input mode:</strong> RGB images (grayscale variants performed worse).</li>
        <li><strong>Hardware:</strong> CPU and GPU (CUDA) runs for comparison.</li>
      </ul>
    </div>

    <!-- BEST CONFIG -->
    <div class="section-card">
      <h2>Best-Performing Configuration</h2>
      <p>
        The final go-to configuration for this project is:
      </p>
      <ul>
        <li>Pretrained AlexNet backbone.</li>
        <li>Custom 2-class classifier head.</li>
        <li>Optimizer: <strong>Adam</strong>.</li>
        <li>Learning rate: <strong>1e-4</strong>.</li>
        <li>Batch size: <strong>32</strong>.</li>
        <li>Device: <strong>GPU</strong> (Colab CUDA).</li>
      </ul>
      <p>
        This setup achieved the best combination of high validation accuracy, fast convergence,
        and a relatively small gap between training and validation performance.
        Quantitative results and plots are summarized on the
        <a href="results.html">Results &amp; Takeaways</a> page.
      </p>
    </div>

    <footer>
      PawSense AI · AlexNet Technical Approach
    </footer>

  </div>
</body>
</html>
